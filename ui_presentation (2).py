# -*- coding: utf-8 -*-
"""ui presentation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f99RxdLy8G0POKmjrGUg0KaSLIk6ySJM
"""

!pip install streamlit pyngrok -q

from pyngrok import ngrok
import os
import threading
import time
import random
import string
from google.colab import userdata
import subprocess

# Terminate any existing ngrok tunnels more aggressively
try:
    subprocess.run(["killall", "ngrok"], capture_output=True, text=True)
    print("Attempted to kill all ngrok processes.")
    time.sleep(5) # Give processes time to terminate
except Exception as e:
    print(f"Could not kill ngrok processes using killall: {e}. Proceeding without killing.")

ngrok.kill() # pyngrok's kill method as a fallback
time.sleep(10) # Increased delay after killing


# Get the ngrok authtoken from Colab secrets
# Make sure you have added NGROK_AUTH_TOKEN to Colab's Secret Manager
try:
    ngrok_auth_token = userdata.get('NGROK_AUTH_TOKEN')
    if ngrok_auth_token:
        ngrok.set_auth_token(ngrok_auth_token)
        os.environ['NGROK_AUTH_TOKEN'] = ngrok_auth_token # set as env var for ! commands
        print("Ngrok authtoken set from Colab secrets.")
    else:
        print("NGROK_AUTH_TOKEN not found in Colab secrets. Please add it.")
        print("You can add secrets by clicking the 'ðŸ”‘' icon in the left sidebar.")
except Exception as e:
    print(f"Error retrieving NGROK_AUTH_TOKEN from Colab secrets: {e}")
    print("Please add NGROK_AUTH_TOKEN to Colab's Secret Manager.")

# Get email credentials from Colab secrets and set as environment variables
try:
    sender_email = userdata.get('EMAIL_SENDER')
    app_password = userdata.get('EMAIL_APP_PASSWORD')
    if sender_email and app_password:
        os.environ['EMAIL_SENDER'] = sender_email
        os.environ['EMAIL_APP_PASSWORD'] = app_password
        print("Email credentials loaded from Colab secrets and set as environment variables.")
    else:
        print("EMAIL_SENDER or EMAIL_APP_PASSWORD not found in Colab secrets. Email sending may not work.")
        print("You can add secrets by clicking the 'ðŸ”‘' icon in the left sidebar.")
except Exception as e:
    print(f"Error retrieving email credentials from Colab secrets: {e}")
    print("Please add EMAIL_SENDER and EMAIL_APP_PASSWORD to Colab's Secret Manager.")


# Starting  a ngrok tunnel to the Streamlit port (default is 8501)
try:
    print("Attempting to start ngrok tunnel...")
    ngrok_tunnel = ngrok.connect(addr="8501", proto="http")
    print("Ngrok tunnel established at:", ngrok_tunnel.public_url)

    # Runs the Streamlit app in the background
    print("Starting Streamlit app...")
    !streamlit run app.py &>/dev/null&
    time.sleep(10) # Increased sleep time
    print("You can access your Streamlit application using the ngrok tunnel URL above.")

except Exception as e:
    print(f"Failed to start ngrok tunnel: {e}")
    print("Please ensure your NGROK_AUTH_TOKEN is correct, and that no other ngrok processes are running.")
    print("If the issue persists, try running 'ngrok kill' in a separate cell and then rerun this cell.")

import nltk

print("Attempting to download 'punkt' and 'punkt_tab'...")

try:
    nltk.download('punkt')
    print("'punkt' downloaded successfully.")
except Exception as e:
    print(f"Failed to download 'punkt': {e}")

try:
    nltk.download('punkt_tab')
    print("'punkt_tab' downloaded successfully (if available as a separate package).")
except Exception as e:
    print(f"Could not download 'punkt_tab' explicitly: {e}. It might be included in 'punkt'.")

print("NLTK download attempts finished. Please run the ngrok cell again.")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import json
# import base64
# import io
# from collections import Counter
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.decomposition import LatentDirichletAllocation, NMF
# import time
# import multiprocessing as mp
# import nltk
# from nltk.corpus import stopwords
# from nltk.tokenize import word_tokenize
# import string
# import re
# import smtplib
# from email.mime.text import MIMEText
# from email.mime.multipart import MIMEMultipart # Import MIMEMultipart
# from email.mime.base import MIMEBase # Import MIMEBase
# from email import encoders # Import encoders
# import os # Import os to access environment variables
# import altair as alt # Import altair for charting
# 
# # Add the default NLTK data path to ensure resources are found
# nltk.data.path.append('/root/nltk_data')
# 
# # Downloading NLTK resources
# try:
#     stopwords.words('english')
# except LookupError:
#     nltk.download('stopwords')
# except Exception as e:
#     st.warning(f"NLTK stopwords download failed: {e}")
# try:
#     word_tokenize("hello world")
# except LookupError:
#     nltk.download('punkt')
# except Exception as e:
#     st.warning(f"NLTK punkt tokenizer download failed: {e}")
# 
# 
# #  function to clean text (repeated for clarity)
# def clean_text_singleprocess(text):
#     if isinstance(text, str):
#         stop_words = set(stopwords.words('english'))
#         text = text.lower()
#         text = re.sub(f'[{re.escape(string.punctuation)}â€“â€”]', '', text)
#         text = re.sub(r'\d+', '', text)
#         words = word_tokenize(text)
#         words = [word for word in words if word not in stop_words and word.strip()]
#         return ' '.join(words)
#     return ''
# 
# #function for multiprocessing text cleaning (repeated for clarity)
# def clean_text_chunk(text_chunk):
#     stop_words = set(stopwords.words('english'))
#     cleaned_chunk = []
#     for text in text_chunk:
#         if isinstance(text, str):
#             text = text.lower()
#             # Include hyphen and similar characters in punctuation removal
#             text = re.sub(f'[{re.escape(string.punctuation)}â€“â€”]', '', text)
#             text = re.sub(r'\d+', '', text)
#             words = word_tokenize(text)
#             words = [word for word in words if word not in stop_words and word.strip()]
#             cleaned_chunk.append(' '.join(words))
#         else:
#             cleaned_chunk.append('')
#     return cleaned_chunk
# 
# #function to format topic modeling results for download (repeated for clarity)
# def format_topic_results(model, feature_names, no_top_words, model_name):
#     output = f"""--- {model_name} Topics ---
# 
# """
#     for topic_idx, topic in enumerate(model.components_):
#         output += f"Topic #{topic_idx+1}:\n"
#         output += " ".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]) + "\n\n"
#     return output
# 
# # function to display top words for a topic model (repeated for clarity)
# def display_topics(model, feature_names, no_top_words):
#     for topic_idx, topic in enumerate(model.components_):
#         st.write(f"Topic #{topic_idx+1}:")
#         st.write(" ".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))
#     st.markdown("---") # Add a separator
# 
# # function to create a download link (repeated for clarity)
# def get_download_link(data, filename, text):
#     b64 = base64.b64encode(data).decode()
#     href = f'<a href="data:application/octet-stream;base64,{b64}" download="{filename}">{text}</a>'
#     return href
# 
# 
# st.title(" TOPIC MODELING APPLICATION")
# st.write("This application performs topic modeling on text data using LDA and NMF, provides word count analysis, and compares model performance.")
# 
# # Initialize session state for login if not already present
# if 'logged_in' not in st.session_state:
#     st.session_state['logged_in'] = False
# 
# # --- Login Section ---
# st.sidebar.header("User Login")
# if not st.session_state['logged_in']:
#     email = st.text_input("Email")
#     password = st.text_input("Password", type='password')
# 
#     # Using placeholders SENDER_EMAIL and APP_PASSWORD for demonstration/setup
#     if st.button("Login"):
#         if email == "SENDER_EMAIL" and password == "APP_PASSWORD": # use secrets
#             st.session_state['logged_in'] = True
#             st.success("Logged in successfully!")
#             st.rerun() # Rerun to show the rest of the app
#         else:
#             st.session_state['logged_in'] = False
#             st.error("Invalid email or password")
# else:
#     st.sidebar.success("Logged in successfully!")
# 
# 
# # --- Main Application Content (Shown only after login) ---
# if st.session_state.get('logged_in', False):
#     # --- Data Upload Section ---
#     st.sidebar.header("Data Upload")
#     st.header("Upload your data")
#     uploaded_file = st.file_uploader("Upload your data (CSV)", type=["csv"])
# 
#     if uploaded_file is not None:
#         try:
#             df = pd.read_csv(uploaded_file)
#             st.session_state['df'] = df # Store DataFrame in session state
#             st.success("File uploaded successfully!")
#             with st.expander("Data Preview"):
#                 st.write(df.head())
#         except Exception as e:
#             st.session_state['df'] = None
#             st.error(f"Error loading file: {e}")
# 
#     # --- Debugging: Display relevant session state keys ---
#     with st.expander("Debugging: Check Analysis Results in Session State"):
#         st.write("Status of session state keys required for Email section:")
#         st.write(f"'word_counts' in st.session_state: {'word_counts' in st.session_state}")
#         st.write(f"'lda_model' in st.session_state: {'lda_model' in st.session_state}")
#         st.write(f"'nmf_model' in st.session_state: {'nmf_model' in st.session_state}")
#         st.write(f"'single_process_time' in st.session_state: {'single_process_time' in st.session_state}")
#         st.write(f"'multi_process_time' in st.session_state: {'multi_process_time' in st.session_state}")
#         if 'cleaned_text_series' in st.session_state and st.session_state['cleaned_text_series'] is not None:
#              st.write(f"'cleaned_text_series' is not None")
#         else:
#              st.write(f"'cleaned_text_series' is None or not in session_state")
# 
#     # --- End Debugging Section ---
# 
#     if 'df' in st.session_state and st.session_state['df'] is not None:
#         df = st.session_state['df']
# 
#         # --- Data Cleaning and Word Count Section ---
#         st.sidebar.header("Data Cleaning and Word Count Analysis")
#         st.header("Clean Data and Analyze Word Count")
#         text_column = st.selectbox("Select the column containing text data:", df.columns, key='clean_text_column')
# 
#         if st.button("Clean Data and Analyze Word Count", key='run_cleaning'):
#             with st.spinner("Cleaning data and counting words..."):
#                 stop_words = set(stopwords.words('english'))
#                 cleaned_text_series = df[text_column].apply(clean_text_singleprocess)
#                 st.session_state['cleaned_text_series'] = cleaned_text_series
# 
#                 all_words = ' '.join(cleaned_text_series).split()
#                 word_counts = Counter(all_words)
#                 st.session_state['word_counts'] = word_counts
# 
#             st.success("Data cleaned and word count performed successfully!")
#             with st.expander("Word Count Results"):
#                 st.write("Top 20 most frequent words:")
#                 st.write(word_counts.most_common(20))
# 
#         # --- Multiprocessing Comparison Section ---
#         st.sidebar.header("Multiprocessing Performance Comparison")
#         st.header("Multiprocessing Performance Comparison")
#         st.write("Compare text cleaning time with and without multiprocessing.")
#         text_column_mp = st.selectbox("Select the column for multiprocessing comparison:", df.columns, key='mp_text_column_select')
# 
#         if st.button("Run Multiprocessing Comparison", key='run_multiprocessing'):
#             if text_column_mp:
#                 # Single-process
#                 with st.spinner("Running single-process text cleaning..."):
#                     start_time_single = time.time()
#                     cleaned_text_single = df[text_column_mp].apply(clean_text_singleprocess)
#                     end_time_single = time.time()
#                     single_process_time = end_time_single - start_time_single
#                     st.session_state['single_process_time'] = single_process_time
#                 st.success(f"Single-process cleaning finished in: {single_process_time:.4f} seconds")
# 
#                 # Multiprocessing
#                 with st.spinner("Running multiprocessing text cleaning..."):
#                     num_processes = mp.cpu_count() if mp.cpu_count() > 1 else 1
#                     st.write(f"Using {num_processes} processes.")
#                     text_data = df[text_column_mp].tolist()
#                     chunk_size = len(text_data) // num_processes + 1
#                     chunks = [text_data[i:i + chunk_size] for i in range(0, len(text_data), chunk_size)]
# 
#                     start_time_multi = time.time()
#                     with mp.Pool(processes=num_processes) as pool:
#                          cleaned_chunks = pool.map(clean_text_chunk, chunks)
#                     end_time_multi = time.time()
#                     multi_process_time = end_time_multi - start_time_multi
#                     st.session_state['multi_process_time'] = multi_process_time
#                 st.success(f"Multiprocessing cleaning finished in: {multi_process_time:.4f} seconds")
# 
#                 with st.expander("Multiprocessing Performance Results"):
#                     st.subheader("Performance Comparison")
#                     st.write(f"Single-process time: {single_process_time:.4f} seconds")
#                     st.write(f"Multiprocessing time: {multi_process_time:.4f} seconds")
#                     if single_process_time is not None and multi_process_time is not None and single_process_time > 0 and multi_process_time > 0:
#                         speedup = single_process_time / multi_process_time
#                         st.write(f"Speedup: {speedup:.2f}x")
#                         # Create and display the bar chart
#                         time_data = pd.DataFrame({
#                             'Process Type': ['Single-process', 'Multiprocessing'],
#                             'Time (seconds)': [single_process_time, multi_process_time]
#                         })
#                         chart = alt.Chart(time_data).mark_bar().encode(
#                             x='Process Type',
#                             y='Time (seconds)',
#                             tooltip=['Process Type', 'Time (seconds)']
#                         ).properties(
#                             title='Text Cleaning Time Comparison'
#                         )
#                         st.altair_chart(chart, use_container_width=True)
# 
# 
#         # --- Topic Modeling Section (LDA and NMF) ---
#         st.sidebar.header("Topic Modeling (LDA and NMF)")
#         st.header("Topic Modeling (LDA and NMF)")
#         st.write("Perform topic modeling on the cleaned text data.")
# 
#         if 'cleaned_text_series' in st.session_state and st.session_state['cleaned_text_series'] is not None:
#             col1_tm, col2_tm = st.columns(2)
#             with col1_tm:
#                 num_topics_lda = st.number_input("Number of topics for LDA:", min_value=2, value=5, step=1, key='num_topics_lda')
#             with col2_tm:
#                 num_topics_nmf = st.number_input("Number of topics for NMF:", min_value=2, value=5, step=1, key='num_topics_nmf')
# 
#             if st.button("Run Topic Modeling", key='run_topic_modeling'):
#                 with st.spinner("Running topic modeling..."):
#                     cleaned_text_list = st.session_state['cleaned_text_series'].tolist()
# 
#                     # Create TF-IDF features
#                     tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')
#                     tfidf = tfidf_vectorizer.fit_transform(cleaned_text_list)
#                     st.session_state['tfidf_vectorizer'] = tfidf_vectorizer
# 
#                     # Train LDA model
#                     start_time_lda = time.time()
#                     lda_model = LatentDirichletAllocation(n_components=num_topics_lda, random_state=42)
#                     lda_model.fit(tfidf)
#                     end_time_lda = time.time()
#                     lda_time = end_time_lda - start_time_lda
#                     st.session_state['lda_model'] = lda_model
#                     st.session_state['lda_time'] = lda_time
# 
#                     # Train NMF model
#                     start_time_nmf = time.time()
#                     nmf_model = NMF(n_components=num_topics_nmf, random_state=42, init='nndsvd')
#                     nmf_model.fit(tfidf)
#                     end_time_nmf = time.time()
#                     nmf_time = end_time_nmf - start_time_nmf
#                     st.session_state['nmf_model'] = nmf_model
#                     st.session_state['nmf_time'] = nmf_time
# 
#                 st.success("Topic modeling complete!")
#                 with st.expander("LDA Topic Results"):
#                     if 'lda_model' in st.session_state and 'tfidf_vectorizer' in st.session_state:
#                         st.subheader("LDA Topic Modeling Results")
#                         display_topics(st.session_state['lda_model'], st.session_state['tfidf_vectorizer'].get_feature_names_out(), 10)
# 
#                 with st.expander("NMF Topic Results"):
#                     if 'nmf_model' in st.session_state and 'tfidf_vectorizer' in st.session_state:
#                         st.subheader("NMF Topic Modeling Results")
#                         display_topics(st.session_state['nmf_model'], st.session_state['tfidf_vectorizer'].get_feature_names_out(), 10)
# 
#                 with st.expander("LDA vs NMF Time Comparison"):
#                     if 'lda_time' in st.session_state and 'nmf_time' in st.session_state:
#                         st.subheader("Model Training Time Comparison")
#                         lda_time = st.session_state['lda_time']
#                         nmf_time = st.session_state['nmf_time']
#                         st.write(f"LDA Training Time: {lda_time:.4f} seconds")
#                         st.write(f"NMF Training Time: {nmf_time:.4f} seconds")
#                         if lda_time is not None and nmf_time is not None and lda_time > 0 and nmf_time > 0:
#                             if lda_time < nmf_time:
#                                 st.success(f"LDA was faster than NMF by {nmf_time - lda_time:.4f} seconds.")
#                                 st.info(f"NMF took {nmf_time / lda_time:.2f} times longer than LDA.")
#                             elif nmf_time < lda_time:
#                                 st.success(f"NMF was faster than LDA by {lda_time - nmf_time:.4f} seconds.")
#                                 st.info(f"LDA took {lda_time / nmf_time:.2f} times longer than NMF.")
#                             else:
#                                 st.info("LDA and NMF training times were approximately equal.")
#                         else:
#                              st.info("Training times are zero or non-positive, cannot compute ratio.")
# 
#                         # Create and display the bar chart for LDA vs NMF times
#                         model_time_data = pd.DataFrame({
#                             'Model Type': ['LDA', 'NMF'],
#                             'Time (seconds)': [lda_time, nmf_time]
#                         })
#                         model_chart = alt.Chart(model_time_data).mark_bar().encode(
#                             x='Model Type',
#                             y='Time (seconds)',
#                             tooltip=['Model Type', 'Time (seconds)']
#                         ).properties(
#                             title='LDA vs NMF Time Comparison'
#                         )
#                         st.altair_chart(model_chart, use_container_width=True)
# 
# 
#         else:
#             st.info("Please clean the data first to run topic modeling.")
# 
# 
#         # --- Download Results Section ---
#         st.sidebar.header("Download Results")
#         st.header("Download Results")
#         st.write("Download the generated analysis results.")
# 
#         col_dl1, col_dl2, col_dl3, col_dl4 = st.columns(4)
# 
#         with col_dl1:
#              if 'word_counts' in st.session_state and isinstance(st.session_state['word_counts'], Counter):
#                 word_counts_dict = dict(st.session_state['word_counts'].most_common()) # Get all counts as dict
#                 word_counts_json = json.dumps(word_counts_dict, indent=4)
#                 st.markdown(get_download_link(word_counts_json.encode(), "word_counts.json", "Word Count (JSON)"), unsafe_allow_html=True)
#              else:
#                  st.info("Word Count not available.")
# 
#         with col_dl2:
#             if 'lda_model' in st.session_state and 'nmf_model' in st.session_state and 'tfidf_vectorizer' in st.session_state:
#                 lda_model = st.session_state['lda_model']
#                 nmf_model = st.session_state['nmf_model']
#                 tfidf_vectorizer = st.session_state['tfidf_vectorizer']
#                 tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()
#                 no_top_words = 10 # Use the same number as in display
# 
#                 lda_output_string = format_topic_results(lda_model, tfidf_feature_names, no_top_words, "LDA")
#                 nmf_output_string = format_topic_results(nmf_model, tfidf_feature_names, no_top_words, "NMF")
#                 all_topics_output = lda_output_string + "\n" + nmf_output_string
#                 st.markdown(get_download_link(all_topics_output.encode(), "topic_modeling_results.txt", "Topics (Text)"), unsafe_allow_html=True)
#             else:
#                 st.info("Topic Models not available.")
# 
#         with col_dl3:
#             if 'single_process_time' in st.session_state and 'multi_process_time' in st.session_state:
#                 single_process_time = st.session_state['single_process_time']
#                 multi_process_time = st.session_state['multi_process_time']
#                 time_comparison_string = f"Single-process cleaning time: {single_process_time:.4f} seconds\n"
#                 time_comparison_string += f"Multiprocessing cleaning time: {multi_process_time:.4f} seconds\n"
#                 if single_process_time is not None and multi_process_time is not None and single_process_time > 0 and multi_process_time > 0:
#                      speedup = single_process_time / multi_process_time
#                      time_comparison_string += f"Speedup: {speedup:.2f}x\n"
#                 st.markdown(get_download_link(time_comparison_string.encode(), "multiprocessing_times.txt", "MP Times (Text)"), unsafe_allow_html=True)
#             else:
#                 st.info("MP Times not available.")
# 
#         with col_dl4:
#             if 'cleaned_text_series' in st.session_state and st.session_state['cleaned_text_series'] is not None:
#                 cleaned_text_df = pd.DataFrame({'cleaned_text': st.session_state['cleaned_text_series']})
#                 csv_buffer = io.StringIO()
#                 cleaned_text_df.to_csv(csv_buffer, index=False)
#                 csv_string = csv_buffer.getvalue()
#                 st.markdown(get_download_link(csv_string.encode(), "cleaned_text.csv", "Cleaned Text (CSV)"), unsafe_allow_html=True)
#             else:
#                 st.info("Cleaned Text not available.")
# 
# 
#         # --- Email Results Section ---
#         st.sidebar.header("Send Results via Email")
#         st.header("Send Results via Email")
#         st.write("Send a summary of the results via email.")
# 
#         if 'word_counts' in st.session_state or \
#            'lda_model' in st.session_state or \
#            'nmf_model' in st.session_state or \
#            'single_process_time' in st.session_state or \
#            'multi_process_time' in st.session_state or \
#            'cleaned_text_series' in st.session_state: # Added cleaned_text_series to check if any results are available
# 
#             recipient_email = st.text_input("Recipient Email Address", key='recipient_email')
#             email_subject = st.text_input("Email Subject (Optional)", value="NLP Topic Modeling Results", key='email_subject')
# 
#             if st.button("Send Email", key='send_email_button'):
#                 if not recipient_email:
#                     st.warning("Please enter a recipient email address.")
#                 else:
#                     st.info("Preparing email...")
# 
#                     # Create the multipart message
#                     msg = MIMEMultipart()
#                     msg['Subject'] = email_subject
#                     msg['From'] = os.environ.get('EMAIL_SENDER') # Get sender email from environment variables
#                     msg['To'] = recipient_email
# 
#                     # Add a plain text body to the email
#                     body = "Please find the analysis results attached."
#                     msg.attach(MIMEText(body, 'plain'))
# 
#                     # Attach Word Count results as CSV
#                     if 'word_counts' in st.session_state and isinstance(st.session_state['word_counts'], Counter):
#                         word_counts_df = pd.DataFrame.from_dict(st.session_state['word_counts'], orient='index', columns=['Count'])
#                         word_counts_csv = word_counts_df.to_csv(index=True)
# 
#                         part = MIMEText(word_counts_csv, 'csv')
#                         part.add_header('Content-Disposition', 'attachment', filename='word_counts.csv')
#                         msg.attach(part)
#                     else:
#                         st.warning("Word Count results not available to attach.")
# 
#                     # Attaching Cleaned Text as CSV
#                     if 'cleaned_text_series' in st.session_state and st.session_state['cleaned_text_series'] is not None:
#                         cleaned_text_df = pd.DataFrame({'cleaned_text': st.session_state['cleaned_text_series']})
#                         cleaned_text_csv = cleaned_text_df.to_csv(index=False)
# 
#                         part = MIMEText(cleaned_text_csv, 'csv')
#                         part.add_header('Content-Disposition', 'attachment', filename='cleaned_text.csv')
#                         msg.attach(part)
#                     else:
#                         st.warning("Cleaned Text not available to attach.")
# 
#                     # Attaching Topic Modeling Results (LDA and NMF) as text file
#                     if 'lda_model' in st.session_state and 'nmf_model' in st.session_state and 'tfidf_vectorizer' in st.session_state:
#                         lda_model = st.session_state['lda_model']
#                         nmf_model = st.session_state['nmf_model']
#                         tfidf_vectorizer = st.session_state['tfidf_vectorizer']
#                         tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()
#                         no_top_words = 10 # Use the same number as in display
# 
#                         lda_output_string = format_topic_results(lda_model, tfidf_feature_names, no_top_words, "LDA")
#                         nmf_output_string = format_topic_results(nmf_model, tfidf_feature_names, no_top_words, "NMF")
#                         all_topics_output = lda_output_string + "\n" + nmf_output_string
# 
#                         part = MIMEText(all_topics_output, 'plain')
#                         part.add_header('Content-Disposition', 'attachment', filename='topic_modeling_results.txt')
#                         msg.attach(part)
#                     else:
#                         st.warning("Topic Modeling results not available to attach.")
# 
#                     # Attaching Multiprocessing Performance Times as text file
#                     if 'single_process_time' in st.session_state and 'multi_process_time' in st.session_state:
#                         single_process_time = st.session_state['single_process_time']
#                         multi_process_time = st.session_state['multi_process_time']
#                         time_comparison_string = f"Single-process cleaning time: {single_process_time:.4f} seconds\n"
#                         time_comparison_string += f"Multiprocessing cleaning time: {multi_process_time:.4f} seconds\n"
#                         if single_process_time is not None and multi_process_time is not None and single_process_time > 0 and multi_process_time > 0:
#                             speedup = single_process_time / multi_process_time
#                             time_comparison_string += f"Speedup: {speedup:.2f}x\n"
# 
#                         part = MIMEText(time_comparison_string, 'plain')
#                         part.add_header('Content-Disposition', 'attachment', filename='multiprocessing_times.txt')
#                         msg.attach(part)
#                     else:
#                         st.warning("Multiprocessing times not available to attach.")
# 
#                     # Attaching LDA and NMF Training Time Comparison as text file
#                     if 'lda_time' in st.session_state and 'nmf_time' in st.session_state:
#                         lda_time = st.session_state['lda_time']
#                         nmf_time = st.session_state['nmf_time']
#                         model_time_comparison_string = f"LDA Training Time: {lda_time:.4f} seconds\n"
#                         model_time_comparison_string += f"NMF Training Time: {nmf_time:.4f} seconds\n"
#                         if lda_time is not None and nmf_time is not None and lda_time > 0 and nmf_time > 0:
#                             if lda_time < nmf_time:
#                                 model_time_comparison_string += f"LDA was faster than NMF by {nmf_time - lda_time:.4f} seconds.\n"
#                                 model_time_comparison_string += f"NMF took {nmf_time / lda_time:.2f} times longer than LDA.\n"
#                             elif nmf_time < lda_time:
#                                 model_time_comparison_string += f"NMF was faster than LDA by {lda_time - nmf_time:.4f} seconds.\n"
#                                 model_time_comparison_string += f"LDA took {lda_time / nmf_time:.2f} times longer than NMF.\n"
#                             else:
#                                 model_time_comparison_string += "LDA and NMF training times were approximately equal.\n"
#                         else:
#                              model_time_comparison_string += "Training times are zero or non-positive, cannot compute ratio.\n"
# 
#                         part = MIMEText(model_time_comparison_string, 'plain')
#                         part.add_header('Content-Disposition', 'attachment', filename='model_training_times.txt')
#                         msg.attach(part)
#                     else:
#                         st.warning("LDA and NMF training times not available to attach.")
# 
# 
#                     # --- Email Sending Logic ---
#                     try:
#                         sender_email = os.environ.get('EMAIL_SENDER') # Get sender email from environment variables
#                         sender_password = os.environ.get('EMAIL_APP_PASSWORD') # Get app password from environment variables
#                         smtp_server = "smtp.gmail.com" # Example for Gmail
#                         smtp_port = 587 # Example SMTP port
# 
#                         if not sender_email or not sender_password:
#                              st.error("Email credentials not found in environment variables. Please set EMAIL_SENDER and EMAIL_APP_PASSWORD in the Colab cell before running the app.")
#                         else:
#                             with smtplib.SMTP(smtp_server, smtp_port) as server:
#                                 server.starttls() # Secure the connection
#                                 server.login(sender_email, sender_password)
#                                 server.sendmail(sender_email, recipient_email, msg.as_string())
# 
#                             st.success(f"Results sent to {recipient_email} successfully!")
# 
#                     except Exception as e:
#                         st.error(f"Failed to send email: {e}")
# 
#                     # --- End of Email Sending Logic ---
# 
#         else:
#             st.info("Run the analysis steps first to generate results before sending an email.")